{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP0YrHbcIwd4UMPb5z/9Cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gavaskare/gavaskar/blob/main/IITP_Gavaskar_YTTL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers sentencepiece deepmultilingualpunctuation\n",
        "\n",
        "!pip install youtube-transcript-api spacy deep-translator\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "5pwkGT3XQJ_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from deep_translator import GoogleTranslator\n",
        "import textwrap\n",
        "import re\n",
        "\n",
        "# --- NEW IMPORTS for Advanced Punctuation and Summarization ---\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load spaCy for sentence splitting of the final summary\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# --- NEW: Load AI Models (this may download models on the first run) ---\n",
        "print(\"Loading AI models, this might take a moment on the first run...\")\n",
        "punc_model = PunctuationModel()\n",
        "# Using a smaller, efficient model for summarization\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "print(\"Models loaded successfully!\")\n",
        "# --- END NEW ---\n",
        "\n",
        "\n",
        "def get_video_id(youtube_link):\n",
        "    \"\"\"Extracts the video ID from a YouTube URL.\"\"\"\n",
        "    if \"v=\" in youtube_link:\n",
        "        return youtube_link.split(\"v=\")[1].split(\"&\")[0]\n",
        "    elif \"youtu.be/\" in youtube_link:\n",
        "        return youtube_link.split(\"youtu.be/\")[1].split(\"?\")[0]\n",
        "    return None\n",
        "\n",
        "def get_transcript(video_id):\n",
        "    \"\"\"Fetches the transcript and combines it into a single text block.\"\"\"\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        # Combine all text parts into one single string\n",
        "        full_transcript = \" \".join([d['text'] for d in transcript_list])\n",
        "        return full_transcript\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching transcript: {e}\"\n",
        "\n",
        "# --- NEW and IMPROVED: Punctuation and Capitalization Function ---\n",
        "def enhance_transcript(text):\n",
        "    \"\"\"Restores punctuation and capitalizes standalone 'i' using an AI model.\"\"\"\n",
        "    # Restore punctuation\n",
        "    punctuated_text = punc_model.restore_punctuation(text)\n",
        "\n",
        "    # Capitalize standalone 'i' using regex for better accuracy\n",
        "    corrected_text = re.sub(r'\\b i \\b', ' I ', punctuated_text)\n",
        "    return corrected_text\n",
        "# --- END NEW ---\n",
        "\n",
        "\n",
        "# --- NEW and IMPROVED: Abstractive Summarization Function ---\n",
        "def summarize_abstractively(text, max_summary_length=150, min_summary_length=50):\n",
        "    \"\"\"\n",
        "    Generates a concise, abstractive summary using a transformer model.\n",
        "    The model creates new sentences to capture the key points.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # The summarizer pipeline returns a list with a dictionary\n",
        "        summary_result = summarizer(text, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)\n",
        "        summary_text = summary_result[0]['summary_text']\n",
        "\n",
        "        # Use spaCy to split the generated summary into distinct sentences (key points)\n",
        "        doc = nlp(summary_text)\n",
        "        return [sent.text.strip() for sent in doc.sents]\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during summarization: {e}\")\n",
        "        return [\"Could not generate a summary due to an error.\"]\n",
        "# --- END NEW ---\n",
        "\n",
        "\n",
        "def translate_points(points, target_lang):\n",
        "    \"\"\"Translates a list of text points to the target language.\"\"\"\n",
        "    if not points or points[0].startswith(\"Could not\"):\n",
        "        return points # Return original points if they are an error message\n",
        "    try:\n",
        "        return [GoogleTranslator(source='auto', target=target_lang).translate(p) for p in points]\n",
        "    except Exception as e:\n",
        "        return [f\"Translation error: {e}\"]\n",
        "\n",
        "def format_text_for_display(text, words_per_line=20):\n",
        "    \"\"\"Formats text to wrap at a certain width for better console display.\"\"\"\n",
        "    # A simple approximation for wrapping width\n",
        "    return textwrap.fill(text, width=words_per_line * 6)\n",
        "\n",
        "def main():\n",
        "    print(\"üé• Welcome to the AI-Powered YouTube Summary Chatbot üé•\")\n",
        "    link = input(\"Enter the YouTube video URL: \").strip()\n",
        "    video_id = get_video_id(link)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüìÑ Fetching transcript...\")\n",
        "    raw_transcript = get_transcript(video_id)\n",
        "    if raw_transcript.startswith(\"Error\"):\n",
        "        print(raw_transcript)\n",
        "        return\n",
        "\n",
        "    print(\"\\n‚ú® Enhancing transcript with punctuation & capitalization...\")\n",
        "    enhanced_transcript = enhance_transcript(raw_transcript)\n",
        "\n",
        "    print(\"\\n--- Full Enhanced Transcript ---\\n\")\n",
        "    print(format_text_for_display(enhanced_transcript))\n",
        "    print(\"\\n‚úÖ Transcript processed successfully!\\n\")\n",
        "\n",
        "    print(\"\\nüß† Generating a concise summary...\")\n",
        "    key_points = summarize_abstractively(enhanced_transcript)\n",
        "    if not key_points:\n",
        "        print(\"Could not summarize the transcript.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Key Points in English ---\")\n",
        "    for i, point in enumerate(key_points, 1):\n",
        "        print(format_text_for_display(f\"{i}. {point}\"))\n",
        "        print() # Add a blank line for readability\n",
        "\n",
        "    print(\"\\nüåê Translate summary to a local Indian language?\")\n",
        "    print(\"Available options: Hindi, Tamil, Telugu, Bengali\")\n",
        "    lang_input = input(\"Enter language name or type 'no' to skip: \").strip().lower()\n",
        "    lang_map = {\"hindi\": \"hi\", \"tamil\": \"ta\", \"telugu\": \"te\", \"bengali\": \"bn\"}\n",
        "\n",
        "    if lang_input == \"no\":\n",
        "        print(\"\\nNo translation selected. Exiting chatbot. üëã\")\n",
        "    elif lang_input in lang_map:\n",
        "        print(f\"\\nüîÑ Translating summary to {lang_input.capitalize()}...\")\n",
        "        translated = translate_points(key_points, lang_map[lang_input])\n",
        "        print(f\"\\n--- Key Points in {lang_input.capitalize()} ---\")\n",
        "        for i, point in enumerate(translated, 1):\n",
        "            print(format_text_for_display(f\"{i}. {point}\"))\n",
        "            print()\n",
        "    else:\n",
        "        print(\"\\nInvalid language input. Exiting. üëã\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "0Dq_Mg5T67dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmZE2_oAPnSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RT9oLi_k7ILz"
      }
    }
  ]
}